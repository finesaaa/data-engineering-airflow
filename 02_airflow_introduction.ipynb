{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Apache Airflow","metadata":{}},{"cell_type":"markdown","source":"## Introduction to Apache Airflow \n\nApache Airflow is an open-source platform designed to create, schedule, and manage data workflows automatically. Initially developed by the team at Airbnb in 2014, Apache Airflow has since become one of the most popular tools in the data analysis and Big Data processing ecosystem.\n\nApache Airflow in data analysis brings several significant benefits, including:\n\n- Operational Efficiency: By automating workflows, the time and effort required to execute routine tasks can be significantly reduced, enhancing overall operational efficiency.\n\n- Consistency: Apache Airflow ensures that workflows are executed consistently according to predefined definitions, reducing the risk of human errors and improving result accuracy.\n\n- Scalability: With the ability to handle complex workflows and large scales, Apache Airflow enables organizations to grow alongside their data expansion without sacrificing performance.\n\n- Flexibility: This platform allows users to easily customize workflows to meet changing business needs, facilitating quick adaptation to environmental changes.\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Apache Airflow Components","metadata":{}},{"cell_type":"markdown","source":"To support the workflow management, Apache airflow has many components around. \n\n![airflow component.png](https://airflow.apache.org/docs/apache-airflow/2.0.1/_images/arch-diag-basic.png)\n\nThe configuration of Apache Airflow and the creation of data pipelines often fall under the responsibility of a data engineer. The configuration process of Airflow can be done through Airflow.cfg, while the data pipeline (DAG) can be managed via the Airflow User Interface. Additionally, a DAG has direct associations with the following components:\n\n- Scheduler: Responsible for scheduling and triggering the execution of tasks.\n- Worker: Responsible for executing tasks.\n- Web Server: Allows users to view and manage DAGs.\n- Metadata: Stores information about the entire workflow that has been created.\n\nBy understanding these fundamental elements, you will see that Apache Airflow plays a crucial role in organizing scheduled workflows and tasks.","metadata":{}},{"cell_type":"markdown","source":"## Directed Acrylic Graph(DAG)","metadata":{}},{"cell_type":"markdown","source":"In Apache Airflow, Task workflows is defined as a DAG (Directed Acrylic Graph). Simply put, a DAG is a collection of tasks to be executed. By using a DAG, you can arrange the sequence and dependencies of each task.\n\n\n![basic dag](https://airflow.apache.org/docs/apache-airflow/stable/_images/basic-dag.png)\n\nHere's a simple DAG example with 4 tasks — a, b, c, d — indicating the sequence and dependencies of each task:\n\n- Task A will be executed first.\n- Tasks B and C will be executed after Task A is completed.\n- Task D will be executed after both Tasks B and C are completed.\n\nBefore we jump into DAG creation, note that apahce airflow will automate the scheduling process. Scheduling process in apache airflow will performed in CRON syntax. We will discuss about this first. ","metadata":{}},{"cell_type":"markdown","source":"## CRON-Syntax","metadata":{}},{"cell_type":"markdown","source":"Cron syntax is a format used to define schedules for executing commands or scripts automatically on Unix-like operating systems. It consists of five fields that represent minute, hour, day of the month, month, and day of the week, respectively. Each field is separated by spaces. Here's a simple explanation of each field:\n\n1. Minute (0–59): Defines the minute of the hour when the command will be executed.\n2. Hour (0–23): Defines the hour of the day when the command will be executed.\n3. Day of the month (1–31): Defines the day of the month when the command will be executed.\n4. Month (1–12): Defines the month of the year when the command will be executed.\n5. Day of the week (0–6 or 7): Defines the day of the week when the command will be executed. (0 or 7 represents Sunday, and 1 represents Monday, and so on.)\n\nUsing these fields, users can specify precise schedules for running tasks at specific times or intervals.","metadata":{}},{"cell_type":"markdown","source":"#### ┌───────────── minute (0–59)\n#### │ ┌───────────── hour (0–23)\n#### │ │ ┌───────────── day of the month (1–31)\n#### │ │ │ ┌───────────── month (1–12)\n#### │ │ │ │ ┌───────────── day of the week (0–6) (Sunday to Saturday;7 is also Sunday on some systems)\n#### │ │ │ │ │                                   \n#### │ │ │ │ │\n#### │ │ │ │ │\n#### * * * * * \n","metadata":{}},{"cell_type":"markdown","source":"For example: \n\n- 0 2 * * *: Executes the task every day at 2 AM.\n- 30 8 * * 1–5: Executes the task every Monday to Friday at 8:30 AM.\n- 0 0 1 * *: Executes the task at midnight on the first day of every month.\n- 0 0 * * 6: Executes the task every Saturday at midnight.\n\nAfter know the scheduling syntax used in apache airflow. Let's move into DAG creation.","metadata":{}},{"cell_type":"markdown","source":"### DAG instance","metadata":{}},{"cell_type":"markdown","source":"There are several ways to create DAG. Most common way is using constructor. One thing you have to know that when you declaring a DAG, you can customize the information, description or configuration in your DAG using `default_args`. `default_args` is parameter when when you declaring a DAG but it stores any setup or information regarding your DAG. You can see list argument you can set in the (official documentation)[https://airflow.apache.org/docs/apache-airflow/1.10.12/tutorial.html] . Since there are so many arguments you can set, to avoid redundancy, most common way is storing the default_args into dictionaries. ","metadata":{}},{"cell_type":"code","source":"!pip install apache-airflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from airflow import DAG\nfrom datetime import datetime, timedelta, timezone\n\ndefault_args = {\n    'owner': 'Algoritma',\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5)\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-21T07:34:56.055352Z","iopub.execute_input":"2024-03-21T07:34:56.056733Z","iopub.status.idle":"2024-03-21T07:34:56.064086Z","shell.execute_reply.started":"2024-03-21T07:34:56.056686Z","shell.execute_reply":"2024-03-21T07:34:56.062696Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Code above is creating `default_args` dictionary. This `default_args` dictionary is used for configuring the default settings and behavior for a Directed Acyclic Graph (DAG) in Apache Airflow, which is a platform used for orchestrating complex workflows. Let's break down each key-value pair:\n\n1. `'owner': 'Algoritma'`: This specifies the owner of the DAG, providing a way to attribute responsibility for the DAG to a specific person or team.\n\n2. `'retries': 3`: This specifies the number of retries that should be attempted in case of task failures. In this case, it's set to 3, meaning a task will be retried three times if it fails.\n\n3. `'retry_delay': timedelta(minutes=5)`: This determines the delay between retries in case of task failures. Here, it's set to 5 minutes, meaning there will be a 5-minutes pause between a failed task and its retry.\n\nLet's continue to how to declaring DAG. \n\nWhen we came into declaring DAG, there are several ways in declaring dag. One of these is using @dag decorator. in simple way, decorator is design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. In previous module, you already learn what is function in python. \n\nNow let's move into the declaring DAG code. ","metadata":{}},{"cell_type":"code","source":"@dag(dag_id='trending_youtube_dag_v1',\n    default_args=default_args,\n    description='A pipeline to fetch trending YouTube videos',\n    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n    schedule_interval='0 10 * * *',\n    catchup=False\n)\ndef trending_youtube_dag():\n    '''\n    This is youtube trending dag, we will define the task in the next section\n    '''\n\n\ndag = trending_youtube_dag()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"@dag(...): This is a decorator used to define a DAG in Apache Airflow. It takes several arguments:\n\ndag_id: Specifies the identifier for the DAG, which is used to uniquely identify it within Airflow.\n\ndefault_args: This parameter specifies the default arguments for the DAG, which include settings like owner, start date, retries, etc. default_args is the dictionary defined earlier in the code snippet.\n\ndescription=Provides a description for the DAG, describing its purpose or functionality.\n\nstart_date=: Specifies the start date for the DAG. Here, it's set to May 7, 2023, with a timezone offset of 7 hours.\n\n`schedule_interval`: Defines the schedule interval for the DAG. In this case, it's set to run at 10:00 AM every day ('0 10 * * *' is a cron expression we have learn earlier).\n\n`catchup`: Specifies whether Airflow should backfill or catch up on any missed DAG runs. Setting it to False means Airflow will only consider future scheduled runs from the start date.\n\n`def trending_youtube_dag()`:: This defines the function trending_youtube_dag, which serves as the entry point for the DAG. Inside this function, you would define the tasks and their dependencies.\n\n`dag = trending_youtube_dag()`: This line calls the trending_youtube_dag function, creating an instance of the DAG. This instance is stored in the variable dag, which can then be used to interact with the DAG within the Airflow environment.\n\nNow we already done declaring DAG. But DAGs are nothing without Tasks to run. ","metadata":{}},{"cell_type":"markdown","source":"## Tasks and It's Dependencies","metadata":{}},{"cell_type":"markdown","source":"In Airflow, a task serves as the fundamental unit of execution. These tasks are organized into Directed Acyclic Graphs (DAGs), and dependencies are established between them to indicate the desired order of execution. There are three basic types of tasks:\n\n- Operators, predefined task templates that you can string together quickly to build most parts of your DAGs. example: PythonOperator, BaseOperator, EmailOperator.\n- Sensors, a special subclass of Operators which are entirely about waiting for an external event to happen.\n- A TaskFlow-decorated `@task`, which is a custom Python function packaged up as a Task.\n\nWe will declare a Task-Flow using @task decorator.  \n","metadata":{}},{"cell_type":"code","source":"@task()\ndef fetch_trending_videos(region_code: str, file_path: str):\n    '''\n    function to be used for fetching trending videos\n    '''\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When you declaring a task using decorator, you have to concise that you are using decorator as-well for declaring a dag. Now let's see full code of our DAG. ","metadata":{}},{"cell_type":"code","source":"@dag(dag_id='trending_youtube_dag_v1',\n    default_args=default_args,\n    description='A pipeline to fetch trending YouTube videos',\n    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n    schedule_interval='0 10 * * *',\n    catchup=False\n)\ndef trending_youtube_dag():\n    '''\n    This is youtube trending dag, we will define the task in the next section\n    '''\n    @task()\n    def fetch_trending_videos(region_code: str, file_path: str):\n        '''\n        function to be used for fetching trending videos\n        '''\n\n\ndag = trending_youtube_dag()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> note: It is important to arrange the indentations, since Python will detect indentation as execution block or inner function. ","metadata":{}},{"cell_type":"markdown","source":"From the code above we already set the task `fetch_trending_videos` for `trending_youtube_dag`. When you have more than one Task in one DAG, you can easily add new @task and the function with the correct indentation. ","metadata":{}},{"cell_type":"code","source":"@dag(dag_id='trending_youtube_dag_v1',\n    default_args=default_args,\n    description='A pipeline to fetch trending YouTube videos',\n    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n    schedule_interval='0 10 * * *',\n    catchup=False\n)\ndef trending_youtube_dag():\n    '''\n    This is youtube trending dag, we will define the task in the next section\n    '''\n    @task()\n    def fetch_trending_videos(region_code: str, file_path: str):\n        '''\n        function to be used for fetching trending videos\n        '''\n    \n    @task()\n    def data_processing(source_file_path: str, target_file_path: str):\n        '''\n        Function to be used for preprocess the data.\n        '''\n\ndag = trending_youtube_dag()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next things to declare inside the DAG is Task-flow. You can add task-flow inside the @dag function. Usually it defined after all tasks is defined. In our case, The flow we want to create is \n\n1. fetch the trending videos\n2. Process fetched data. \n\nFirst things to do is create a new variable that save call our task function. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"file_path = '/opt/airflow/dags/tmp_file.json'\nfetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\nprocessed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\ndata_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We already set the path we want. Now we have to declare the dependency between task. In Airflow, dependency of our task is defined using `>>` operator. In our case, we want the `data_preprocessing_task` executed after `fetch_trensing video_task`. So the dependency should be.\n\n```\nfetch_trending_videos_task >> data_processing_task\n```\n\nNow our code should be look like this. ","metadata":{}},{"cell_type":"code","source":"@dag(dag_id='trending_youtube_dag_v1',\n    default_args=default_args,\n    description='A pipeline to fetch trending YouTube videos',\n    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n    schedule_interval='0 10 * * *',\n    catchup=False\n)\ndef trending_youtube_dag():\n    '''\n    This is youtube trending dag, we will define the task in the next section\n    '''\n    @task()\n    def fetch_trending_videos(region_code: str, file_path: str):\n        '''\n        function to be used for fetching trending videos\n        '''\n    \n    @task()\n    def data_processing(source_file_path: str, target_file_path: str):\n        '''\n        Function to be used for preprocess the data.\n        '''\n    \n    file_path = '/opt/airflow/dags/tmp_file.json'\n    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n    \n    fetch_trending_videos_task >> data_processing_task\n    \ndag = trending_youtube_dag()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}