{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect DB (SQLite) to Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "This notebook will perform how to connect and combine SQLite, a lightweight and efficient relational database, with Airflow, open source platform for worfklow automation and scheduling.\n",
    "\n",
    "Integrating SQLite with Airflow allows us to interact with our SQLite database, run quesries and load or export data from an Airflow DAG.\n",
    "\n",
    "- **Introduction to SQLite**\n",
    "  - SQLite Definition\n",
    "  - Define SQLite in Python and Airflow\n",
    "- **Connect SQLite to Airflow**\n",
    "  - Creating DAG to fetch youtube API, Preprocess the data, and load to SQLite database.\n",
    "  - Creating task sequence and scheduling in DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import sqlite3 # sqlite3 database\n",
    "import pandas as pd # pandas for dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite Definition\n",
    "\n",
    "SQLite is one of the most popular and easy-to-use relational database. It is an open-source library with zero configuration and does not require installation.\n",
    "\n",
    "Why use SQLite:\n",
    "\n",
    "- SQLite is an open-source software. The software does not require any license after installation.\n",
    "- SQLite is serverless as it doesn't need a different server process or system to operate.\n",
    "- SQLite facilitates you to work on multiple databases on the same session simultaneously, thus making it flexible.\n",
    "- SQLite is a cross-platform DBMS that can run on all platforms, including macOS, Windows, etc.\n",
    "- SQLite doesn't require any configuration. It needs no setup or administration.\n",
    "\n",
    "For these reasons we use SQLite as our database project since it is really easy to config and doesn't need any installation to use it with Python.\n",
    "\n",
    "*Note: For new airflow project that will use SQLite as database and want to run it in Docker. Please use `docker-compose.yaml` from this project that has configure for SQLite database in airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SQLite in Python and Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite Database Path in Airflow\n",
    "\n",
    "First thing to ensure is the path of SQLite database stored in Airflow project. To make sure/modify the path:\n",
    "\n",
    "- Open the `docker-compose.yaml`.\n",
    "- Search `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN`. It stores the path of SQLite database.\n",
    "- The path of the SQLite database should be `sqlite:////opt/airflow/db/airflow.db`. So if we want to create SQLite database, it should be in `db/airflow.db` in project folder or `/opt/airflow/db/airflow.db` in Docker Airflow project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQLite Database with Python\n",
    "\n",
    "Creating SQLite database can be performed in python using `sqlite3` built-in library (no need installation).\n",
    "\n",
    "The steps to creating database in python:\n",
    "\n",
    "- Import `sqlite3` library\n",
    "- Create python connection to SQLite database with\n",
    "  ```python\n",
    "  conn = sqlite3.connect(\"database path\")\n",
    "  ```\n",
    "\n",
    "If the database already exist, than python will connect with existing database.\n",
    "\n",
    "Example to creating new SQLite database in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = 'db/trial.db'\n",
    "conn = sqlite3.connect(db_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create database named `trial.db` (if didn't exist) in `db` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert to Database from DataFrame\n",
    "\n",
    "`pandas` has provide method to insert dataframe directly into database. The method To insert dataframe into database is `df.to_sql()`. The needed input is the name of the target table and the dataframe itself, the other inputs can be adjusted as needed preference.\n",
    "\n",
    "```python\n",
    "df.to_sql(name,con,if_exist,index)\n",
    "```\n",
    "\n",
    "**Parameter:**\n",
    "\n",
    "- `name`: Name of SQL table\n",
    "- `con`: Connection object\n",
    "- `if_exist`: How to behave if table already exist.\n",
    "  - `fail`: Raise a ValueError\n",
    "  - `replace`: Drop the table before inserting new values\n",
    "  - `append`: Insert new values to the existing table.\n",
    "- `index`: Write Dataframe index as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we have dataframe below and want to insert into `users` table in `trial.db` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name\n",
       "0  User 1\n",
       "1  User 2\n",
       "2  User 3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into users table\n",
    "df.to_sql(name='users', con=conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method will return the number of data successfully inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into the Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways to show our data in the database table.\n",
    "1. `pandas` way: we can use pandas to query all the data from certain table\n",
    "2. SQLite viewer way: we can use `SQLite` extension from VSCode to look dive into our data in database.\n",
    "\n",
    "We will breakdown briefly each way in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pandas way\n",
    "\n",
    "To get (commonly called `fetch`) the data from database, we can utilize the `conn` object we have created that connect python with the database.\n",
    "\n",
    "The method to fetch data using pandas from database is `pd.read_sql_query(sql, con)` that need query and connection object as input.\n",
    "\n",
    "**Parameter:**\n",
    "- `sql`: SQL Query\n",
    "- `con`: Connection object\n",
    "\n",
    "Let's fetch all data in our `users` table in `trial.db` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>User 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>User 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>User 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    name\n",
       "0      0  User 1\n",
       "1      1  User 2\n",
       "2      2  User 3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(sql = \"SELECT * FROM USERS\",\n",
    "                con = conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SQLite Viewer Way\n",
    "\n",
    "SQLite Viewer is an extension in VSCode that provide functionality to open and see SQLite database. The steps to use SQLite viewer are:\n",
    "\n",
    "1. Install SQLite Viewer Extension\n",
    "   First, search \"SQLite Viewer\" in Extensions tab and click \"install\"\n",
    "   ![](assets/sqlite_viewer.png)\n",
    "2. Search and double click the target db to open in VSCode Explorer, in this case we want to open `db/trial.db`.\n",
    "3. It will open new window that show our data within the table.\n",
    "   ![](assets/trialdb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect SQLite to Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have know how to connect to database and insert data into it from pandas dataframe. Now let's create functionality to insert processed youtube data in `tmp_file_processed.json` to SQLite database.\n",
    "\n",
    "First, we need to load our data from the json file and stored it as dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>trendingAt</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>defaultAudioLanguage</th>\n",
       "      <th>caption</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>durationSec</th>\n",
       "      <th>category</th>\n",
       "      <th>thumbnailUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lRbxVxbbavU</td>\n",
       "      <td>2024-04-19T10:06:41Z</td>\n",
       "      <td>HIGHLIGHTS! Indonesia (1) vs (0) Australia | A...</td>\n",
       "      <td>2024-04-18T15:26:56Z</td>\n",
       "      <td>UCeM5Nksgv9_FXTuZ8jkPJPg</td>\n",
       "      <td>RCTI - ENTERTAINMENT</td>\n",
       "      <td>Permainan yang sangat menegangkan dari kedua N...</td>\n",
       "      <td>RCTI, Timnas Indonesia, Garuda indonesia, Pial...</td>\n",
       "      <td>id</td>\n",
       "      <td>False</td>\n",
       "      <td>3310552</td>\n",
       "      <td>47123</td>\n",
       "      <td>7503</td>\n",
       "      <td>1202</td>\n",
       "      <td>Sports</td>\n",
       "      <td>https://i.ytimg.com/vi/lRbxVxbbavU/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bCZZ7zyGtYE</td>\n",
       "      <td>2024-04-19T10:06:41Z</td>\n",
       "      <td>Manchester City VS Real Madrid | Highlights Li...</td>\n",
       "      <td>2024-04-18T00:00:19Z</td>\n",
       "      <td>UC_vsErcsq56hOscPHkG-aVw</td>\n",
       "      <td>SCTV</td>\n",
       "      <td>Tonton full highlights https://www.vidio.com/w...</td>\n",
       "      <td>sctv, indonesia, movies, eufa champions league...</td>\n",
       "      <td>id</td>\n",
       "      <td>False</td>\n",
       "      <td>1102265</td>\n",
       "      <td>12179</td>\n",
       "      <td>2554</td>\n",
       "      <td>72</td>\n",
       "      <td>Sports</td>\n",
       "      <td>https://i.ytimg.com/vi/bCZZ7zyGtYE/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g7pMevhVeSM</td>\n",
       "      <td>2024-04-19T10:06:41Z</td>\n",
       "      <td>HAPPY ASMARA Feat. GILGA SAHID - LAMUNAN | Fea...</td>\n",
       "      <td>2024-04-17T03:00:08Z</td>\n",
       "      <td>UCEdO8A1SpMMF47LuGGV1nMQ</td>\n",
       "      <td>MUARA BINTANG</td>\n",
       "      <td>HAPPY ASMARA Feat. GILGA SAHID - LAMUNAN | Fea...</td>\n",
       "      <td>indosiar, jtv, sctv, rcti, viral, terpopuler, ...</td>\n",
       "      <td>id</td>\n",
       "      <td>True</td>\n",
       "      <td>1120373</td>\n",
       "      <td>102722</td>\n",
       "      <td>30719</td>\n",
       "      <td>306</td>\n",
       "      <td>Music</td>\n",
       "      <td>https://i.ytimg.com/vi/g7pMevhVeSM/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaYJwfPx3oY</td>\n",
       "      <td>2024-04-19T10:06:41Z</td>\n",
       "      <td>HIGHLIGHT QATAR VS INDONESIA| AFC U23 ASIAN CU...</td>\n",
       "      <td>2024-04-15T18:40:43Z</td>\n",
       "      <td>UCE9-bV_MCGgLnH7v4HSApDg</td>\n",
       "      <td>Official RCTI</td>\n",
       "      <td>Ayo saksikan Full Episode program RCTI lainnya...</td>\n",
       "      <td>RCTI, Official RCTI, Sinetron Terbaru, Film te...</td>\n",
       "      <td>id</td>\n",
       "      <td>False</td>\n",
       "      <td>2280191</td>\n",
       "      <td>14492</td>\n",
       "      <td>7933</td>\n",
       "      <td>1050</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>https://i.ytimg.com/vi/BaYJwfPx3oY/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1ie232_ylQ</td>\n",
       "      <td>2024-04-19T10:06:41Z</td>\n",
       "      <td>1 HARI MENJADI GANGSTER - GTA 5 ROLEPLAY</td>\n",
       "      <td>2024-04-18T00:01:06Z</td>\n",
       "      <td>UCqPN9LAkx20aBWOafwvQB-Q</td>\n",
       "      <td>Jelool</td>\n",
       "      <td>Subscribe ► https://www.youtube.com/c/Jelool\\n...</td>\n",
       "      <td>executive roleplay, misi gta 5, balap liar gta...</td>\n",
       "      <td>id</td>\n",
       "      <td>False</td>\n",
       "      <td>415469</td>\n",
       "      <td>39132</td>\n",
       "      <td>7887</td>\n",
       "      <td>1987</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>https://i.ytimg.com/vi/D1ie232_ylQ/sddefault.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId            trendingAt  \\\n",
       "0  lRbxVxbbavU  2024-04-19T10:06:41Z   \n",
       "1  bCZZ7zyGtYE  2024-04-19T10:06:41Z   \n",
       "2  g7pMevhVeSM  2024-04-19T10:06:41Z   \n",
       "3  BaYJwfPx3oY  2024-04-19T10:06:41Z   \n",
       "4  D1ie232_ylQ  2024-04-19T10:06:41Z   \n",
       "\n",
       "                                               title           publishedAt  \\\n",
       "0  HIGHLIGHTS! Indonesia (1) vs (0) Australia | A...  2024-04-18T15:26:56Z   \n",
       "1  Manchester City VS Real Madrid | Highlights Li...  2024-04-18T00:00:19Z   \n",
       "2  HAPPY ASMARA Feat. GILGA SAHID - LAMUNAN | Fea...  2024-04-17T03:00:08Z   \n",
       "3  HIGHLIGHT QATAR VS INDONESIA| AFC U23 ASIAN CU...  2024-04-15T18:40:43Z   \n",
       "4           1 HARI MENJADI GANGSTER - GTA 5 ROLEPLAY  2024-04-18T00:01:06Z   \n",
       "\n",
       "                  channelId          channelTitle  \\\n",
       "0  UCeM5Nksgv9_FXTuZ8jkPJPg  RCTI - ENTERTAINMENT   \n",
       "1  UC_vsErcsq56hOscPHkG-aVw                  SCTV   \n",
       "2  UCEdO8A1SpMMF47LuGGV1nMQ         MUARA BINTANG   \n",
       "3  UCE9-bV_MCGgLnH7v4HSApDg         Official RCTI   \n",
       "4  UCqPN9LAkx20aBWOafwvQB-Q                Jelool   \n",
       "\n",
       "                                         description  \\\n",
       "0  Permainan yang sangat menegangkan dari kedua N...   \n",
       "1  Tonton full highlights https://www.vidio.com/w...   \n",
       "2  HAPPY ASMARA Feat. GILGA SAHID - LAMUNAN | Fea...   \n",
       "3  Ayo saksikan Full Episode program RCTI lainnya...   \n",
       "4  Subscribe ► https://www.youtube.com/c/Jelool\\n...   \n",
       "\n",
       "                                                tags defaultAudioLanguage  \\\n",
       "0  RCTI, Timnas Indonesia, Garuda indonesia, Pial...                   id   \n",
       "1  sctv, indonesia, movies, eufa champions league...                   id   \n",
       "2  indosiar, jtv, sctv, rcti, viral, terpopuler, ...                   id   \n",
       "3  RCTI, Official RCTI, Sinetron Terbaru, Film te...                   id   \n",
       "4  executive roleplay, misi gta 5, balap liar gta...                   id   \n",
       "\n",
       "   caption  viewCount  likeCount  commentCount  durationSec       category  \\\n",
       "0    False    3310552      47123          7503         1202         Sports   \n",
       "1    False    1102265      12179          2554           72         Sports   \n",
       "2     True    1120373     102722         30719          306          Music   \n",
       "3    False    2280191      14492          7933         1050  Entertainment   \n",
       "4    False     415469      39132          7887         1987         Gaming   \n",
       "\n",
       "                                       thumbnailUrl  \n",
       "0  https://i.ytimg.com/vi/lRbxVxbbavU/sddefault.jpg  \n",
       "1  https://i.ytimg.com/vi/bCZZ7zyGtYE/sddefault.jpg  \n",
       "2  https://i.ytimg.com/vi/g7pMevhVeSM/sddefault.jpg  \n",
       "3  https://i.ytimg.com/vi/BaYJwfPx3oY/sddefault.jpg  \n",
       "4  https://i.ytimg.com/vi/D1ie232_ylQ/sddefault.jpg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_file_path = 'dags/tmp_file_processed.json'\n",
    "\n",
    "df = pd.read_json(source_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read the json file. We need to load our dataframe to SQLite database. First, let's create or connect to our database, `db/airflow.db` that have been define in `docker-compose.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection\n",
    "database = 'db/airflow.db'\n",
    "conn = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to insert our `df` dataframe into `trending_videos` table in `db/airflow.db`.\n",
    "\n",
    "To achieve this goals, we will use `df.to_sql()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "table_name = 'trending_videos'\n",
    "df.to_sql(name = table_name, con=conn, \n",
    "            if_exists='append', \n",
    "            index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have insert clean trending videos data to airflow.db. If we arrange the steps to insert the data to `load_to_sqlite` function, should be look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(source_file_path: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Loads the processed data to SQLite.\n",
    "    \n",
    "    Args:\n",
    "        source_file_path: A string representing the path to the file to be loaded.\n",
    "        table_name: A string representing the name of the table to load the data to.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the json file to sqlite\n",
    "    df = pd.read_json(source_file_path)\n",
    "    database = \"/opt/airflow/db/airflow.db\"\n",
    "    conn = sqlite3.connect(database)\n",
    "    \n",
    "    # Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "    df.to_sql(name=table_name, con=conn, if_exists='append', index=False)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    # Log the job results\n",
    "    print(\"Done Created DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DAG\n",
    "\n",
    "Integrating SQLite with Airflow allows us to interact with our SQLite database, run quesries and load or export data from an Airflow DAG.\n",
    "\n",
    "We need to create these following tasks to run in Airflow DAG:\n",
    "- `fetch_trending_videos` task, fetch youtube trending videos as we have disscussed in first module `01_fetching_data_api`.\n",
    "- `data_processing` task, to clean and process fetching result before load it to database SQLite.\n",
    "- `load_to_sqlite` task, load data to database SQLite after data processing.\n",
    "\n",
    "We will complete and continue our DAG from `02_airflow_introduction` module. Our current DAG:\n",
    "\n",
    "```python\n",
    "@dag(dag_id='trending_youtube_dag_v1',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        '''\n",
    "        Function to be used for preprocess the data.\n",
    "        '''\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch_trending_videos\n",
    "\n",
    "We need to move our code to fetching trending video in `01_fetching_data_api` module to `fetch_treding_videos` task.\n",
    "\n",
    "Let's complete the `fetch_trending_videos` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_trending_videos(region_code: str, file_path: str):\n",
    "    '''\n",
    "    function to be used for fetching trending videos\n",
    "    '''\n",
    "\n",
    "    # Load API key from .env file\n",
    "    load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "    api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "    # Create YouTube API client\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # Fetch videos until max_results is reached or there are no more results\n",
    "    videos_list = []\n",
    "    next_page_token = \"\"\n",
    "    while len(videos_list) < max_results and next_page_token is not None:\n",
    "        # Make API request for videos\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region_code,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token,\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract videos from response\n",
    "        videos = response.get(\"items\", [])\n",
    "\n",
    "        # Update next_page_token for the next API request\n",
    "        next_page_token = response.get(\"nextPageToken\", None)\n",
    "        \n",
    "        # Extract relevant video details and append to list\n",
    "        infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                            'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                    'contentDetails':['duration', 'caption'],\n",
    "                    'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "        now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        for video in videos:\n",
    "            video_details = {\n",
    "                'videoId': video[\"id\"],\n",
    "                'trendingAt': now\n",
    "            }\n",
    "            \n",
    "            for k in infos.keys():\n",
    "                for info in infos[k]:\n",
    "                    # use try-except to handle missing info\n",
    "                    try:\n",
    "                        video_details[info] = video[k][info]\n",
    "                    except KeyError:\n",
    "                        video_details[info] = None\n",
    "                        \n",
    "            videos_list.append(video_details)\n",
    "\n",
    "    # Write fetched videos data to a json file\n",
    "    with open(target_file_path, \"w\") as f:\n",
    "        json.dump(videos_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have create function that perform fetching data from the start until it save certain column/component in json format.\n",
    "\n",
    "Our updated DAG after completing `fetch_trending_videos` should like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dag(dag_id='trending_youtube_dag_v1',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        '''\n",
    "        Function to be used for preprocess the data.\n",
    "        '''\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_processing\n",
    "\n",
    "After completing fetching task, we need to completing and move the data processing code from `01_fetching_data_api` to `trending_youtube_dag_sqlite` DAG.\n",
    "\n",
    "Let's complete our `data_processing` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(source_file_path: str, target_file_path: str):\n",
    "    \n",
    "    \"\"\"Processes the raw data fetched from YouTube.\n",
    "    Args:\n",
    "        source_file_path: A string representing the path to the file to be processed.\n",
    "        target_file_path: A string representing the path to the file to be written.\n",
    "    \"\"\"\n",
    "    # Load the fetched videos data from the json file\n",
    "    with open(source_file_path, 'r') as f:\n",
    "        videos_list = json.load(f)\n",
    "    \n",
    "    # Load the categories dictionary from the json file\n",
    "    with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "        categories = json.load(f)\n",
    "    \n",
    "    # Process the fetched videos data\n",
    "    for video in videos_list:\n",
    "        # Convert ISO 8601 duration to seconds\n",
    "        video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "        del video['duration']\n",
    "        \n",
    "        # Convert tags list to string\n",
    "        video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "        \n",
    "        # Convert categoryId to category based on categories dictionary\n",
    "        video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "        del video['categoryId']\n",
    "\n",
    "        # Parse the thumbnail url\n",
    "        video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "        del video['thumbnails']\n",
    "        \n",
    "        # Convert viewCount, likeCount, and commentCount to integer\n",
    "        video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "        video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "        video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "        \n",
    "        # Convert caption to boolean\n",
    "        video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "    \n",
    "    # Save the processed videos data to a new file\n",
    "    with open(target_file_path, \"w\") as f:\n",
    "        json.dump(videos_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we complete our `data_processing` function. The updated DAG should be look like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dag(dag_id='trending_youtube_dag_v1',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        \"\"\"Processes the raw data fetched from YouTube.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be processed.\n",
    "            target_file_path: A string representing the path to the file to be written.\n",
    "        \"\"\"\n",
    "        # Load the fetched videos data from the json file\n",
    "        with open(source_file_path, 'r') as f:\n",
    "            videos_list = json.load(f)\n",
    "        \n",
    "        # Load the categories dictionary from the json file\n",
    "        with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "            categories = json.load(f)\n",
    "        \n",
    "        # Process the fetched videos data\n",
    "        for video in videos_list:\n",
    "            # Convert ISO 8601 duration to seconds\n",
    "            video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "            del video['duration']\n",
    "            \n",
    "            # Convert tags list to string\n",
    "            video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "            \n",
    "            # Convert categoryId to category based on categories dictionary\n",
    "            video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "            del video['categoryId']\n",
    "\n",
    "            # Parse the thumbnail url\n",
    "            video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "            del video['thumbnails']\n",
    "            \n",
    "            # Convert viewCount, likeCount, and commentCount to integer\n",
    "            video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "            video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "            video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "            \n",
    "            # Convert caption to boolean\n",
    "            video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "        \n",
    "        # Save the processed videos data to a new file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_to_sql\n",
    "\n",
    "After we create `load_to_sqlite` function, we need to create `load_to_sqlite` task and define the sequence of the task. The `load_to_sqlite` will defined after `data_processing_task` task.\n",
    "\n",
    "The updated dag should be look like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dag(dag_id='trending_youtube_dag_v1',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        \"\"\"Processes the raw data fetched from YouTube.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be processed.\n",
    "            target_file_path: A string representing the path to the file to be written.\n",
    "        \"\"\"\n",
    "        # Load the fetched videos data from the json file\n",
    "        with open(source_file_path, 'r') as f:\n",
    "            videos_list = json.load(f)\n",
    "        \n",
    "        # Load the categories dictionary from the json file\n",
    "        with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "            categories = json.load(f)\n",
    "        \n",
    "        # Process the fetched videos data\n",
    "        for video in videos_list:\n",
    "            # Convert ISO 8601 duration to seconds\n",
    "            video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "            del video['duration']\n",
    "            \n",
    "            # Convert tags list to string\n",
    "            video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "            \n",
    "            # Convert categoryId to category based on categories dictionary\n",
    "            video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "            del video['categoryId']\n",
    "\n",
    "            # Parse the thumbnail url\n",
    "            video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "            del video['thumbnails']\n",
    "            \n",
    "            # Convert viewCount, likeCount, and commentCount to integer\n",
    "            video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "            video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "            video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "            \n",
    "            # Convert caption to boolean\n",
    "            video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "        \n",
    "        # Save the processed videos data to a new file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def load_to_sqlite(source_file_path: str, table_name: str):\n",
    "        \"\"\"\n",
    "        Loads the processed data to SQLite.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be loaded.\n",
    "            table_name: A string representing the name of the table to load the data to.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load the data from the json file to sqlite\n",
    "        df = pd.read_json(source_file_path)\n",
    "        database = \"/opt/airflow/db/airflow.db\"\n",
    "        conn = sqlite3.connect(database)\n",
    "        \n",
    "        # Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "        df.to_sql(name=table_name, con=conn, if_exists='append', index=False)\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "        # Log the job results\n",
    "        print(\"Done Created DB\")\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    load_to_sqlite_task = load_to_sqlite(source_file_path=processed_file_path, table_name='trending_videos')\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task >> load_to_sqlite_task\n",
    "    \n",
    "dag = trending_youtube_dag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
