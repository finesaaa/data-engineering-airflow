{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Connect Database SQLite to Airflow**\n",
    "\n",
    "- Part of Data Engineering Airflow Specialization\n",
    "- Course Length: 2 Hours\n",
    "- Last Updated: May 2024\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Developed by [Algoritma](https://algorit.ma/)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    " \n",
    "The coursebook is part of the **Data Engineering Airflow Specialization** prepared by Algoritma. The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "This notebook will perform how to connect and combine SQLite, a lightweight and efficient relational database, with Airflow, open source platform for worfklow automation and scheduling.\n",
    "\n",
    "Integrating SQLite with Airflow allows us to interact with our SQLite database, run quesries and load or export data from an Airflow DAG.\n",
    "\n",
    "- **Introduction to SQLite**\n",
    "  - SQLite Definition\n",
    "  - Define SQLite in Python and Airflow\n",
    "- **Connect SQLite to Airflow**\n",
    "  - Creating DAG to fetch youtube API, Preprocess the data, and load to SQLite database.\n",
    "  - Creating task sequence and scheduling in DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Database SQLite to Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import sqlite3 # sqlite3 database\n",
    "import pandas as pd # pandas for dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary libraries also from module one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import json # handling json file\n",
    "import isodate # handling datetime\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite Definition\n",
    "\n",
    "SQLite is one of the most popular and easy-to-use relational database. It is an open-source library with zero configuration and does not require installation.\n",
    "\n",
    "Why use SQLite:\n",
    "\n",
    "- SQLite is an open-source software. The software does not require any license after installation.\n",
    "- SQLite is serverless as it doesn't need a different server process or system to operate.\n",
    "- SQLite facilitates you to work on multiple databases on the same session simultaneously, thus making it flexible.\n",
    "- SQLite is a cross-platform DBMS that can run on all platforms, including macOS, Windows, etc.\n",
    "- SQLite doesn't require any configuration. It needs no setup or administration.\n",
    "\n",
    "For these reasons we use SQLite as our database project since it is really easy to config and doesn't need any installation to use it with Python.\n",
    "\n",
    "*Note: For new airflow project that will use SQLite as database and want to run it in Docker. Please use `docker-compose.yaml` from this project that has configure for SQLite database in airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SQLite in Python and Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite Database Path in Airflow\n",
    "\n",
    "First thing to ensure is the path of SQLite database stored in Airflow project. To make sure/modify the path:\n",
    "\n",
    "- Open the `docker-compose.yaml`.\n",
    "- Search `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN`. It stores the path of SQLite database.\n",
    "- The path of the SQLite database should be `sqlite:////opt/airflow/db/airflow.db`. So if we want to create SQLite database, it should be in `db/airflow.db` in project folder or `/opt/airflow/db/airflow.db` in Docker Airflow project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQLite Database with Python\n",
    "\n",
    "Creating SQLite database can be performed in python using `sqlite3` built-in library (no need installation).\n",
    "\n",
    "The steps to creating database in python:\n",
    "\n",
    "- Import `sqlite3` library\n",
    "- Create python connection to SQLite database with\n",
    "  ```python\n",
    "  conn = sqlite3.connect(\"database path\")\n",
    "  ```\n",
    "\n",
    "If the database already exist, than python will connect with existing database.\n",
    "\n",
    "Example to creating new SQLite database in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = 'db/trial.db'\n",
    "conn = sqlite3.connect(db_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create database named `trial.db` (if didn't exist) in `db` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert to Database from DataFrame\n",
    "\n",
    "`pandas` has provide method to insert dataframe directly into database. The method To insert dataframe into database is `df.to_sql()`. The needed input is the name of the target table and the dataframe itself, the other inputs can be adjusted as needed preference.\n",
    "\n",
    "```python\n",
    "df.to_sql(name,con,if_exist,index)\n",
    "```\n",
    "\n",
    "**Parameter:**\n",
    "\n",
    "- `name`: Name of SQL table\n",
    "- `con`: Connection object\n",
    "- `if_exist`: How to behave if table already exist.\n",
    "  - `fail`: Raise a ValueError\n",
    "  - `replace`: Drop the table before inserting new values\n",
    "  - `append`: Insert new values to the existing table.\n",
    "- `index`: Write Dataframe index as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we have dataframe below and want to insert into `users` table in `trial.db` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name\n",
       "0  User 1\n",
       "1  User 2\n",
       "2  User 3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into users table\n",
    "df.to_sql(name='users', con=conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method will return the number of data successfully inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into the Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways to show our data in the database table.\n",
    "1. `pandas` way: we can use pandas to query all the data from certain table\n",
    "2. SQLite viewer way: we can use `SQLite` extension from VSCode to look dive into our data in database.\n",
    "\n",
    "We will breakdown briefly each way in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pandas way\n",
    "\n",
    "To get (commonly called `fetch`) the data from database, we can utilize the `conn` object we have created that connect python with the database.\n",
    "\n",
    "The method to fetch data using pandas from database is `pd.read_sql_query(sql, con)` that need query and connection object as input.\n",
    "\n",
    "**Parameter:**\n",
    "- `sql`: SQL Query\n",
    "- `con`: Connection object\n",
    "\n",
    "Let's fetch all data in our `users` table in `trial.db` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>User 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>User 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>User 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    name\n",
       "0      0  User 1\n",
       "1      1  User 2\n",
       "2      2  User 3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(sql = \"SELECT * FROM USERS\",\n",
    "                con = conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SQLite Viewer Way\n",
    "\n",
    "SQLite Viewer is an extension in VSCode that provide functionality to open and see SQLite database. The steps to use SQLite viewer are:\n",
    "\n",
    "1. Install SQLite Viewer Extension\n",
    "   First, search \"SQLite Viewer\" in Extensions tab and click \"install\"\n",
    "   ![](assets/sqlite_viewer.png)\n",
    "2. Search and double click the target db to open in VSCode Explorer, in this case we want to open `db/trial.db`.\n",
    "3. It will open new window that show our data within the table.\n",
    "   ![](assets/trialdb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect SQLite to Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have know how to connect to database and insert data into it from pandas dataframe. Now let's create functionality to insert processed youtube data in `tmp_file_processed.json` to SQLite database.\n",
    "\n",
    "First, we need to load our data from the json file and stored it as dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>trendingAt</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>description</th>\n",
       "      <th>defaultAudioLanguage</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>category</th>\n",
       "      <th>thumbnailUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4ofJpOEXrZs</td>\n",
       "      <td>2024-05-06T00:50:45Z</td>\n",
       "      <td>THE AMAZING DIGITAL CIRCUS - Ep 2: Candy Carri...</td>\n",
       "      <td>2024-05-03T22:00:08Z</td>\n",
       "      <td>GLITCH</td>\n",
       "      <td>The gang are BACK for a WAaAaAaACKY candy fill...</td>\n",
       "      <td>en</td>\n",
       "      <td>37702723</td>\n",
       "      <td>2532340</td>\n",
       "      <td>234462</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>https://i.ytimg.com/vi/4ofJpOEXrZs/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exNL0QTdhWg</td>\n",
       "      <td>2024-05-06T00:50:45Z</td>\n",
       "      <td>NGE-RATING SEMUA ROBOSEN TRANSFORMERS YANG PER...</td>\n",
       "      <td>2024-05-04T11:30:16Z</td>\n",
       "      <td>Medy Renaldy</td>\n",
       "      <td>Di video ini gue bakal rating semua Robot Tran...</td>\n",
       "      <td>id</td>\n",
       "      <td>766791</td>\n",
       "      <td>23009</td>\n",
       "      <td>3467</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>https://i.ytimg.com/vi/exNL0QTdhWg/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zUVcKpUnfq0</td>\n",
       "      <td>2024-05-06T00:50:45Z</td>\n",
       "      <td>ARAFAH NGAKU SIAPA PACARNYA SEKARANG! - OMWEN</td>\n",
       "      <td>2024-05-04T12:00:55Z</td>\n",
       "      <td>WENDI CAGUR</td>\n",
       "      <td>#KOMEDI #WENDICAGUR \\n========================...</td>\n",
       "      <td>None</td>\n",
       "      <td>207703</td>\n",
       "      <td>4991</td>\n",
       "      <td>262</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>https://i.ytimg.com/vi/zUVcKpUnfq0/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AfPbJeTE8dQ</td>\n",
       "      <td>2024-05-06T00:50:45Z</td>\n",
       "      <td>KELUARGA BARU KU PART 5 (TAMAT) - Animasi Sekolah</td>\n",
       "      <td>2024-05-04T11:12:35Z</td>\n",
       "      <td>Dhot Design</td>\n",
       "      <td>Animasi yang menceritakan masa masa sekolah SM...</td>\n",
       "      <td>id</td>\n",
       "      <td>4198713</td>\n",
       "      <td>202320</td>\n",
       "      <td>19570</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>https://i.ytimg.com/vi/AfPbJeTE8dQ/sddefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LCEhBBk080I</td>\n",
       "      <td>2024-05-06T00:50:45Z</td>\n",
       "      <td>skibidi toilet 73 (part 2)</td>\n",
       "      <td>2024-05-03T03:00:06Z</td>\n",
       "      <td>DaFuq!?Boom!</td>\n",
       "      <td>titans confront g-toilet\\n\\nfull-screen versio...</td>\n",
       "      <td>en</td>\n",
       "      <td>22953193</td>\n",
       "      <td>988124</td>\n",
       "      <td>123213</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>https://i.ytimg.com/vi/LCEhBBk080I/sddefault.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId            trendingAt   \n",
       "0  4ofJpOEXrZs  2024-05-06T00:50:45Z  \\\n",
       "1  exNL0QTdhWg  2024-05-06T00:50:45Z   \n",
       "2  zUVcKpUnfq0  2024-05-06T00:50:45Z   \n",
       "3  AfPbJeTE8dQ  2024-05-06T00:50:45Z   \n",
       "4  LCEhBBk080I  2024-05-06T00:50:45Z   \n",
       "\n",
       "                                               title           publishedAt   \n",
       "0  THE AMAZING DIGITAL CIRCUS - Ep 2: Candy Carri...  2024-05-03T22:00:08Z  \\\n",
       "1  NGE-RATING SEMUA ROBOSEN TRANSFORMERS YANG PER...  2024-05-04T11:30:16Z   \n",
       "2      ARAFAH NGAKU SIAPA PACARNYA SEKARANG! - OMWEN  2024-05-04T12:00:55Z   \n",
       "3  KELUARGA BARU KU PART 5 (TAMAT) - Animasi Sekolah  2024-05-04T11:12:35Z   \n",
       "4                         skibidi toilet 73 (part 2)  2024-05-03T03:00:06Z   \n",
       "\n",
       "   channelTitle                                        description   \n",
       "0        GLITCH  The gang are BACK for a WAaAaAaACKY candy fill...  \\\n",
       "1  Medy Renaldy  Di video ini gue bakal rating semua Robot Tran...   \n",
       "2   WENDI CAGUR  #KOMEDI #WENDICAGUR \\n========================...   \n",
       "3   Dhot Design  Animasi yang menceritakan masa masa sekolah SM...   \n",
       "4  DaFuq!?Boom!  titans confront g-toilet\\n\\nfull-screen versio...   \n",
       "\n",
       "  defaultAudioLanguage  viewCount  likeCount  commentCount          category   \n",
       "0                   en   37702723    2532340        234462  Film & Animation  \\\n",
       "1                   id     766791      23009          3467     Entertainment   \n",
       "2                 None     207703       4991           262     Entertainment   \n",
       "3                   id    4198713     202320         19570  Film & Animation   \n",
       "4                   en   22953193     988124        123213     Entertainment   \n",
       "\n",
       "                                       thumbnailUrl  \n",
       "0  https://i.ytimg.com/vi/4ofJpOEXrZs/sddefault.jpg  \n",
       "1  https://i.ytimg.com/vi/exNL0QTdhWg/sddefault.jpg  \n",
       "2  https://i.ytimg.com/vi/zUVcKpUnfq0/sddefault.jpg  \n",
       "3  https://i.ytimg.com/vi/AfPbJeTE8dQ/sddefault.jpg  \n",
       "4  https://i.ytimg.com/vi/LCEhBBk080I/sddefault.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_path = 'dags/tmp_file_processed.json'\n",
    "\n",
    "df = pd.read_json(source_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read the json file. We need to load our dataframe to SQLite database. First, let's create or connect to our database, `db/airflow.db` that have been define in `docker-compose.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection\n",
    "database = 'db/airflow.db'\n",
    "conn = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to insert our `df` dataframe into `trending_videos` table in `db/airflow.db`.\n",
    "\n",
    "To achieve this goals, we will use `df.to_sql()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "table_name = 'trending_videos'\n",
    "df.to_sql(name = table_name, con=conn, \n",
    "            if_exists='append', \n",
    "            index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have insert clean trending videos data to airflow.db. If we arrange the steps to insert the data to `load_to_sqlite` function, should be look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(source_file_path: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Loads the processed data to SQLite.\n",
    "    \n",
    "    Args:\n",
    "        source_file_path: A string representing the path to the file to be loaded.\n",
    "        table_name: A string representing the name of the table to load the data to.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the json file to sqlite\n",
    "    df = pd.read_json(source_file_path)\n",
    "    database = \"/opt/airflow/db/airflow.db\"\n",
    "    conn = sqlite3.connect(database)\n",
    "    \n",
    "    # Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "    df.to_sql(name=table_name, con=conn, if_exists='append', index=False)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    # Log the job results\n",
    "    print(\"Done Created DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DAG\n",
    "\n",
    "Integrating SQLite with Airflow allows us to interact with our SQLite database, run quesries and load or export data from an Airflow DAG.\n",
    "\n",
    "We need to create these following tasks to run in Airflow DAG:\n",
    "- `fetch_trending_videos` task, fetch youtube trending videos as we have disscussed in first module `01_fetching_data_api`.\n",
    "- `data_processing` task, to clean and process fetching result before load it to database SQLite.\n",
    "- `load_to_sqlite` task, load data to database SQLite after data processing.\n",
    "\n",
    "We will complete and continue our DAG from `02_airflow_introduction` module. Our current DAG:\n",
    "\n",
    "```python\n",
    "@dag(dag_id='trending_youtube_dag_sqlite',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        '''\n",
    "        Function to be used for preprocess the data.\n",
    "        '''\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch_trending_videos\n",
    "\n",
    "We need to move our code to fetching trending video in `01_fetching_data_api` module to `fetch_treding_videos` task.\n",
    "\n",
    "Let's complete the `fetch_trending_videos` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_trending_videos(region_code: str, file_path: str):\n",
    "    '''\n",
    "    function to be used for fetching trending videos\n",
    "    '''\n",
    "\n",
    "    # Load API key from .env file\n",
    "    load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "    api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "    # Create YouTube API client\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # Fetch videos until max_results is reached or there are no more results\n",
    "    videos_list = []\n",
    "    next_page_token = \"\"\n",
    "    while len(videos_list) < max_results and next_page_token is not None:\n",
    "        # Make API request for videos\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region_code,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token,\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract videos from response\n",
    "        videos = response.get(\"items\", [])\n",
    "\n",
    "        # Update next_page_token for the next API request\n",
    "        next_page_token = response.get(\"nextPageToken\", None)\n",
    "        \n",
    "        # Extract relevant video details and append to list\n",
    "        infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                            'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                    'contentDetails':['duration', 'caption'],\n",
    "                    'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "        now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        for video in videos:\n",
    "            video_details = {\n",
    "                'videoId': video[\"id\"],\n",
    "                'trendingAt': now\n",
    "            }\n",
    "            \n",
    "            for k in infos.keys():\n",
    "                for info in infos[k]:\n",
    "                    # use try-except to handle missing info\n",
    "                    try:\n",
    "                        video_details[info] = video[k][info]\n",
    "                    except KeyError:\n",
    "                        video_details[info] = None\n",
    "                        \n",
    "            videos_list.append(video_details)\n",
    "\n",
    "    # Write fetched videos data to a json file\n",
    "    with open(target_file_path, \"w\") as f:\n",
    "        json.dump(videos_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have create function that perform fetching data from the start until it save certain column/component in json format.\n",
    "\n",
    "Our updated DAG after completing `fetch_trending_videos` should like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@dag(dag_id='trending_youtube_dag_sqlite',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        '''\n",
    "        Function to be used for preprocess the data.\n",
    "        '''\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_processing\n",
    "\n",
    "After completing fetching task, we need to completing and move the data processing code from `01_fetching_data_api` to `trending_youtube_dag_sqlite` DAG.\n",
    "\n",
    "Let's complete our `data_processing` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(source_file_path: str, target_file_path: str):\n",
    "    \n",
    "    \"\"\"Processes the raw data fetched from YouTube.\n",
    "    Args:\n",
    "        source_file_path: A string representing the path to the file to be processed.\n",
    "        target_file_path: A string representing the path to the file to be written.\n",
    "    \"\"\"\n",
    "    # Load the fetched videos data from the json file\n",
    "    with open(source_file_path, 'r') as f:\n",
    "        videos_list = json.load(f)\n",
    "    \n",
    "    # Load the categories dictionary from the json file\n",
    "    with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "        categories = json.load(f)\n",
    "    \n",
    "    # Process the fetched videos data\n",
    "    for video in videos_list:\n",
    "        # Convert ISO 8601 duration to seconds\n",
    "        video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "        del video['duration']\n",
    "        \n",
    "        # Convert tags list to string\n",
    "        video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "        \n",
    "        # Convert categoryId to category based on categories dictionary\n",
    "        video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "        del video['categoryId']\n",
    "\n",
    "        # Parse the thumbnail url\n",
    "        video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "        del video['thumbnails']\n",
    "        \n",
    "        # Convert viewCount, likeCount, and commentCount to integer\n",
    "        video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "        video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "        video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "        \n",
    "        # Convert caption to boolean\n",
    "        video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "    \n",
    "    # Save the processed videos data to a new file\n",
    "    with open(target_file_path, \"w\") as f:\n",
    "        json.dump(videos_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we complete our `data_processing` function. The updated DAG should be look like below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@dag(dag_id='trending_youtube_dag_sqlite',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        \"\"\"Processes the raw data fetched from YouTube.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be processed.\n",
    "            target_file_path: A string representing the path to the file to be written.\n",
    "        \"\"\"\n",
    "        # Load the fetched videos data from the json file\n",
    "        with open(source_file_path, 'r') as f:\n",
    "            videos_list = json.load(f)\n",
    "        \n",
    "        # Load the categories dictionary from the json file\n",
    "        with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "            categories = json.load(f)\n",
    "        \n",
    "        # Process the fetched videos data\n",
    "        for video in videos_list:\n",
    "            # Convert ISO 8601 duration to seconds\n",
    "            video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "            del video['duration']\n",
    "            \n",
    "            # Convert tags list to string\n",
    "            video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "            \n",
    "            # Convert categoryId to category based on categories dictionary\n",
    "            video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "            del video['categoryId']\n",
    "\n",
    "            # Parse the thumbnail url\n",
    "            video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "            del video['thumbnails']\n",
    "            \n",
    "            # Convert viewCount, likeCount, and commentCount to integer\n",
    "            video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "            video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "            video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "            \n",
    "            # Convert caption to boolean\n",
    "            video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "        \n",
    "        # Save the processed videos data to a new file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task\n",
    "    \n",
    "dag = trending_youtube_dag()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_to_sql\n",
    "\n",
    "After we create `load_to_sqlite` function, we need to create `load_to_sqlite` task and define the sequence of the task. The `load_to_sqlite` will defined after `data_processing_task` task.\n",
    "\n",
    "The updated dag as a final dag should be look like below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@dag(dag_id='trending_youtube_dag_sqlite',\n",
    "    default_args=default_args,\n",
    "    description='A pipeline to fetch trending YouTube videos',\n",
    "    start_date=datetime(2023, 5, 7, tzinfo=timezone(timedelta(hours=7))),\n",
    "    schedule_interval='0 10 * * *',\n",
    "    catchup=False\n",
    ")\n",
    "def trending_youtube_dag():\n",
    "    '''\n",
    "    This is youtube trending dag, we will define the task in the next section\n",
    "    '''\n",
    "    @task()\n",
    "    def fetch_trending_videos(region_code: str, file_path: str):\n",
    "        '''\n",
    "        function to be used for fetching trending videos\n",
    "        '''\n",
    "\n",
    "        # Load API key from .env file\n",
    "        load_dotenv(\"/opt/airflow/dags/.env\")\n",
    "        api_key = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "        # Create YouTube API client\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Fetch videos until max_results is reached or there are no more results\n",
    "        videos_list = []\n",
    "        next_page_token = \"\"\n",
    "        while len(videos_list) < max_results and next_page_token is not None:\n",
    "            # Make API request for videos\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract videos from response\n",
    "            videos = response.get(\"items\", [])\n",
    "\n",
    "            # Update next_page_token for the next API request\n",
    "            next_page_token = response.get(\"nextPageToken\", None)\n",
    "            \n",
    "            # Extract relevant video details and append to list\n",
    "            infos = {'snippet':['title', 'publishedAt', 'channelId', 'channelTitle',\n",
    "                                'description', 'tags', 'thumbnails', 'categoryId', 'defaultAudioLanguage'],\n",
    "                        'contentDetails':['duration', 'caption'],\n",
    "                        'statistics':['viewCount', 'likeCount', 'commentCount']}\n",
    "            now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            for video in videos:\n",
    "                video_details = {\n",
    "                    'videoId': video[\"id\"],\n",
    "                    'trendingAt': now\n",
    "                }\n",
    "                \n",
    "                for k in infos.keys():\n",
    "                    for info in infos[k]:\n",
    "                        # use try-except to handle missing info\n",
    "                        try:\n",
    "                            video_details[info] = video[k][info]\n",
    "                        except KeyError:\n",
    "                            video_details[info] = None\n",
    "                            \n",
    "                videos_list.append(video_details)\n",
    "\n",
    "        # Write fetched videos data to a json file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def data_processing(source_file_path: str, target_file_path: str):\n",
    "        \"\"\"Processes the raw data fetched from YouTube.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be processed.\n",
    "            target_file_path: A string representing the path to the file to be written.\n",
    "        \"\"\"\n",
    "        # Load the fetched videos data from the json file\n",
    "        with open(source_file_path, 'r') as f:\n",
    "            videos_list = json.load(f)\n",
    "        \n",
    "        # Load the categories dictionary from the json file\n",
    "        with open('/opt/airflow/dags/categories.json', 'r') as f:\n",
    "            categories = json.load(f)\n",
    "        \n",
    "        # Process the fetched videos data\n",
    "        for video in videos_list:\n",
    "            # Convert ISO 8601 duration to seconds\n",
    "            video['durationSec'] = int(isodate.parse_duration(video['duration']).total_seconds()) if video['duration'] is not None else None\n",
    "            del video['duration']\n",
    "            \n",
    "            # Convert tags list to string\n",
    "            video['tags'] = ', '.join(video['tags']) if video['tags'] is not None else None\n",
    "            \n",
    "            # Convert categoryId to category based on categories dictionary\n",
    "            video['category'] = categories.get(video['categoryId'], None) if video['categoryId'] is not None else None\n",
    "            del video['categoryId']\n",
    "\n",
    "            # Parse the thumbnail url\n",
    "            video['thumbnailUrl'] = video['thumbnails'].get('standard', {}).get('url', None) if video['thumbnails'] is not None else None\n",
    "            del video['thumbnails']\n",
    "            \n",
    "            # Convert viewCount, likeCount, and commentCount to integer\n",
    "            video['viewCount'] = int(video['viewCount']) if video['viewCount'] is not None else None\n",
    "            video['likeCount'] = int(video['likeCount']) if video['likeCount'] is not None else None\n",
    "            video['commentCount'] = int(video['commentCount']) if video['commentCount'] is not None else None\n",
    "            \n",
    "            # Convert caption to boolean\n",
    "            video['caption'] = True if video['caption'] == 'true' else False if video['caption'] == 'false' else None\n",
    "        \n",
    "        # Save the processed videos data to a new file\n",
    "        with open(target_file_path, \"w\") as f:\n",
    "            json.dump(videos_list, f)\n",
    "    \n",
    "    @task()\n",
    "    def load_to_sqlite(source_file_path: str, table_name: str):\n",
    "        \"\"\"\n",
    "        Loads the processed data to SQLite.\n",
    "        \n",
    "        Args:\n",
    "            source_file_path: A string representing the path to the file to be loaded.\n",
    "            table_name: A string representing the name of the table to load the data to.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load the data from the json file to sqlite\n",
    "        df = pd.read_json(source_file_path)\n",
    "        database = \"/opt/airflow/db/airflow.db\"\n",
    "        conn = sqlite3.connect(database)\n",
    "        \n",
    "        # Append the DataFrame to the existing table if it exists, otherwise create a new table\n",
    "        df.to_sql(name=table_name, con=conn, if_exists='append', index=False)\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "        # Log the job results\n",
    "        print(\"Done Created DB\")\n",
    "    \n",
    "    file_path = '/opt/airflow/dags/tmp_file.json'\n",
    "    fetch_trending_videos_task = fetch_trending_videos(region_code='ID', max_results=200, target_file_path=file_path)\n",
    "    processed_file_path = '/opt/airflow/dags/tmp_file_processed.json'\n",
    "    data_processing_task = data_processing(source_file_path=file_path, target_file_path=processed_file_path)\n",
    "    load_to_sqlite_task = load_to_sqlite(source_file_path=processed_file_path, table_name='trending_videos')\n",
    "    \n",
    "    fetch_trending_videos_task >> data_processing_task >> load_to_sqlite_task\n",
    "    \n",
    "dag = trending_youtube_dag()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
